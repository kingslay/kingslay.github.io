# 深度学习入门

## 一、概念

### 1、人工智能、 机器学习、深度学习的关系

这是现在大家经常混淆的概念，什么叫做人工智能？什么叫做机器学习？什么叫做深度学习？

#### （1）人工智能

**人工智能**（英语：**A**rtificial **I**ntelligence, **AI**）亦称**机器智能**，是指由人工制造出来的系统所表现出来的智能。通常人工智能是指通过普通电脑实现的智能。

#### （2）机器学习

**机器学习**是[人工智能](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD)的一个分支。机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。

机器学习可以分成下面几种类别：

- [监督学习](https://zh.wikipedia.org/wiki/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0)从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括[回归分析](https://zh.wikipedia.org/wiki/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90)和[统计分类](https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%88%86%E7%B1%BB)。
- [无监督学习](https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92)与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有[聚类](https://zh.wikipedia.org/wiki/%E8%81%9A%E7%B1%BB)。
- [半监督学习](https://zh.wikipedia.org/w/index.php?title=%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&action=edit&redlink=1)介于监督学习与无监督学习之间。
- [增强学习](https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。

具体的机器学习算法有：

- 构造[间隔理论](https://zh.wikipedia.org/w/index.php?title=%E9%97%B4%E9%9A%94%E7%90%86%E8%AE%BA&action=edit&redlink=1)分布：[聚类分析](https://zh.wikipedia.org/wiki/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90)和[模式识别](https://zh.wikipedia.org/wiki/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB) 
- [人工神经网络](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)
- [决策树](https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91)
- [感知器](https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8)
- [支持向量机](https://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA)
- [集成学习](https://zh.wikipedia.org/w/index.php?title=%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0&action=edit&redlink=1)[AdaBoost](https://zh.wikipedia.org/wiki/AdaBoost)
- [降维与度量学习](https://zh.wikipedia.org/w/index.php?title=%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0&action=edit&redlink=1)
- [聚类](https://zh.wikipedia.org/wiki/%E8%81%9A%E7%B1%BB)
- [贝叶斯分类器](https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8)
- 构造[条件概率](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87)：[回归分析](https://zh.wikipedia.org/wiki/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90)和[统计分类](https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%88%86%E7%B1%BB)
- [高斯过程回归](https://zh.wikipedia.org/w/index.php?title=Kriging&action=edit&redlink=1)
- [线性判别分析](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90)
- [最近邻居法](https://zh.wikipedia.org/wiki/%E6%9C%80%E8%BF%91%E9%84%B0%E5%B1%85%E6%B3%95)
- [径向基函数核](https://zh.wikipedia.org/wiki/%E5%BE%84%E5%90%91%E5%9F%BA%E5%87%BD%E6%95%B0%E6%A0%B8)
- 通过[再生模型](https://zh.wikipedia.org/w/index.php?title=%E5%86%8D%E7%94%9F%E6%A8%A1%E5%9E%8B&action=edit&redlink=1)构造[概率密度函数](https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0)：
- [最大期望算法](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95)
- [概率图模型](https://zh.wikipedia.org/wiki/%E5%9C%96%E6%A8%A1%E5%BC%8F)：包括[贝叶斯网](https://zh.wikipedia.org/w/index.php?title=%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91&action=edit&redlink=1)和[Markov随机场](https://zh.wikipedia.org/w/index.php?title=Markov%E9%9A%8F%E6%9C%BA%E5%9C%BA&action=edit&redlink=1)
- 近似推断技术：
- [马尔可夫链](https://zh.wikipedia.org/wiki/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE)
- [蒙特卡罗方法](https://zh.wikipedia.org/wiki/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95)
- [变分法](https://zh.wikipedia.org/wiki/%E5%8F%98%E5%88%86%E6%B3%95)
- [最优化](https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BC%98%E5%8C%96)：大多数以上方法，直接或者间接使用最优化算法。 

#### （3）深度学习

**深度学习**（英语：deep learning）是[机器学习](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)拉出的分支，它试图使用包含复杂结构或由多重非[线性变换](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2)构成的多个处理层对数据进行高层抽象的[算法](https://zh.wikipedia.org/wiki/%E7%AE%97%E6%B3%95)；其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。

至今已有数种深度学习框架，如[深度神经网络](https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0#.E6.B7.B1.E5.BA.A6.E7.A5.9E.E7.BB.8F.E7.BD.91.E7.BB.9C)、[卷积神经网络](https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)和[深度置信网络](https://zh.wikipedia.org/w/index.php?title=%E6%B7%B1%E5%BA%A6%E7%BD%AE%E4%BF%A1%E7%BD%91%E7%BB%9C&action=edit&redlink=1)和[递归神经网络](https://zh.wikipedia.org/wiki/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)已被应用[计算机视觉](https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89)、[语音识别](https://zh.wikipedia.org/wiki/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB)、[自然语言处理](https://zh.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86)、音频识别与[生物信息学](https://zh.wikipedia.org/wiki/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6)等领域并获取了极好的效果。

### 2、机器学习解决问题的基本步骤

一般应用机器学习解决实际问题分为4个步骤：

**1**）定义目标问题

目前还没看到有一个机器学习模型适用于解决所有问题，不同问题有各自适用的模型，如图像相关问题有深度学习、推荐相关问题有专门的推荐算法、安全相关问题有异常检测模型等。脱离具体问题去讨论模型是没有意义的。

**2 **)收集数据和特征工程

机器学习是面向数据编程，数据是机器学习的基础。训练模型时，一般会把样本数据拆成两部分，其中大部分(约7成）数据用于训练模型，称其为训练集；另外少部分数据用于测试“模型的好坏”（也称“泛化能力”），称其为测试集。

同一个机器学习算法，好的数据能让其表现更好，差的数据会让模型毫无用处。什么是“好的数据”？并没有统一定义，从结果看，能让模型表现良好的数据就是“好的数据”。一个可行的办法是想象“人”在解决该问题时，会依据哪些数据和特征做决策，然后挑选这些数据和特征作为机器学习模型的输入数据，这个过程就是特征工程。在应用机器学习时，可能需要反复做多次特征工程，特征工程是个试错的过程。　　

**3 **)训练模型和评估模型效果　

利用标注数据，训练模型数据，而一般的步骤是：

a. 从底层存储读取数据

b. 对训练数据进行前向计算

c. 计算训练误差

d. 反向计算梯度，更新网络参数

e. 重复a - d 步，直到模型收敛

测试模型效果，一般测试数据集远小于训练集，这里主要是快速前向计算，一般合并在第一步中。

**4 **)线上应用和持续优化

模型在训练集上性能达标，但在线上环境性能不达标，这一现象被称为“过拟合”。通常的原因是用于训练模型的数据中特征的分布与线上数据偏差太大，此时需提取更具代表性的数据重新训练模型。

模型在线上应用后，需持续跟踪模型的性能表现，机器学习是面向数据编程，如果线上系统上的数据出现了不包含在训练集中的新特征，需要补充新样本，重新训练迭代模型以保证预测效果。

## 二、感知器

**感知器**（英语：Perceptron）一种最简单形式的[前馈神经网络](https://zh.wikipedia.org/wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)，是一种二元[线性分类器](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8)。在人工神经网络领域中，感知机也被指为单层的[人工神经网络](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)(ANN)，以区别于较复杂的[多层感知机](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA)（Multilayer Perceptron）

### 1、概览

感知机是生物[神经细胞](https://zh.wikipedia.org/wiki/%E7%A5%9E%E7%BB%8F%E7%BB%86%E8%83%9E)的简单抽象。神经细胞结构大致可分为：[树突](https://zh.wikipedia.org/wiki/%E6%A0%91%E7%AA%81)、[突触](https://zh.wikipedia.org/wiki/%E7%AA%81%E8%A7%A6)、[细胞体](https://zh.wikipedia.org/w/index.php?title=%E7%BB%86%E8%83%9E%E4%BD%93&action=edit&redlink=1)及[轴突](https://zh.wikipedia.org/wiki/%E8%BD%B4%E7%AA%81)。单个神经细胞可被视为一种只有两种状态的机器——激动时为‘是’，而未激动时为‘否’。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激动，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。

### 2、结构

[![Ncell.png](https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Ncell.png/300px-Ncell.png)](https://zh.wikipedia.org/wiki/File:Ncell.png)

设有n输入的单个感知机（如上图示），a1至an为n维输入[向量](https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F)的各个分量，w1至wn为各个输入分量连接到感知机的权量（或称权值），b为偏置，f(.)为激活函数（又曰激励函数或传递函数），t为标量输出。输出t的数学描述为：


![{\mathbf t=f(\sum _{i=1}^{n}{{w}_{i}{x}_{i}+b})=f(\mathbf {w} ^{T}\mathbf {x} )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/54c7e092c4ad81a12ef8f5ec706f5608bf6e2795)

其中![{\mathbf  {w}}=[w_{1}\;w_{2}\;...\;w_{n}\;b]^{T}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8e814b252d9514474fa0eb70431741b8b0e46a23)，![{\mathbf  {x}}=[x_{1}\;x_{2}\;...\;x_{n}\;1]^{T}](https://wikimedia.org/api/rest_v1/media/math/render/svg/09cd946d4df939cd266dab9d84d47fb485805398)及f(x)为S型激活函数，它将整个一维空间映射到[0,1]或[-1,1]
$$
f(x)=\frac{1}{1+e^{-\alpha x}}(0<f(x)<1)\\f'(x)=\frac{\alpha e^{-\alpha x}}{(1+e^{-\alpha x})^2}=\alpha f(x)[1-f(x)]
$$

### 3、学习算法

#### 梯度下降（BSD）

首先我们来定义输出误差，即对于任意一组权值向量，那它得到的输出和我们预想的输出之间的误差值。

定义误差的方法很多，不同的误差计算方法可以得到不同的权值更新法则，这里我们先用这样的定义：
$$
E(\vec{w})=\frac{1}{2}\sum_{d \in D}(t_d-o_d)^2
$$
上面公式中D代表了所有的输入实例，或者说是样本，d代表了一个样本实例，od表示感知器的输出，td代表我们预想的输出。

这样，我们的目标就明确了，就是想找到一组权值让这个误差的值最小，显然我们用误差对权值求导将是一个很好的选择，导数的意义是提供了一个方向，沿着这个方向改变权值，将会让总的误差变大，更形象的叫它为梯度。
$$
\nabla E(w_i)=\frac{\partial E}{\partial w}=\frac{1}{2}\frac{\partial \sum_{d \in D}(t_d-o_d)^2}{\partial w_i}=\frac{1}{2}\sum_{d \in D}\frac{\partial (t_d-o_d)^2}{\partial w_i}
$$
既然梯度确定了E最陡峭的上升的方向，那么梯度下降的训练法则是：
$$
\vec{w_i} \gets \vec{w_i}+\Delta \vec{w_i}，其中\Delta \vec{w_i}=-\eta \frac{\partial E}{\partial w_i}
$$
#### 

梯度下降是一种重要最优化算法，但在应用它的时候通常会有两个问题：

1）有时收敛过程可能非常慢；

2）如果误差曲面上有多个局极小值，那么不能保证这个过程会找到全局最小值。

#### 随机梯度下降（SGD）

为了解决上面的问题，实际中我们应用的是梯度下降的一种变体被称为随机梯度下降。上面公式中的误差是针对于所有训练样本而得到的，而随机梯度下降的思想是根据每个单独的训练样本来更新权值，这样我们上面的梯度公式就变成了：
$$
\frac{\partial E}{\partial w_i}=\frac{1}{2}\frac{\partial (t-o)^2}{\partial w_i}=-(t-o)\frac{\partial o}{\partial w_i}
$$
经过推导后，我们就可以得到最终的权值更新的公式：
$$
w_i=w_i+\Delta w_i \\      \delta=(t-o)o(1-o) \\      \Delta w_i=\eta \delta x_i
$$
有了上面权重的更新公式后，我们就可以通过输入大量的实例样本，来根据我们预期的结果不断地调整权值，从而最终得到一组权值使得我们的SIGMOID能够对一个新的样本输入得到正确的或无限接近的结果。

随机梯度下降（SGD）在应用它的时候通常会有一个问题：

（1）每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。

所以还有下面两张梯度下降的变种

#### Mini-batch Gradient Descent

（1）这是介于BSD和SGD之间的一种优化[算法](http://lib.csdn.net/base/datastructure)。每次选取一定量的训练样本进行迭代。

（2）从公式上似乎可以得出以下分析：速度比BSD快，比SGD慢；精度比BSD低，比SGD高。

#### 带Mini-batch的SGD

（1）选择n个训练样本（n<m，m为总训练集样本数）

（2）在这n个样本中进行n次迭代，每次使用1个样本

（3）对n次迭代得出的n个gradient进行加权平均再并求和，作为这一次mini-batch下降梯度

（4）不断在训练集中重复以上步骤，直到收敛。



## 三、多层感知器（MLP）

### 1、概览

多层网络，顾名思义就是由多个层结构组成的网络系统，它的每一层都是由若干个神经元结点构成，该层的任意一个结点都与上一层的每一个结点相联，由它们来提供输入，经过计算产生该结点的输出并作为下一层结点的输入。

值得注意的是任何多层的网络结构必须有一输入层、一个输出层。

### 2、结构

![20150128033221168](/Users/kintan/Downloads/20150128033221168.png)

上图是一个3层的网络结构，它由一个输入层、一个输出层和一个隐藏层构成，当然隐藏层的层数可以更多。图像隐藏层的结点$i$与输入层的每一个结点相连，也就是说它接收了一组向量$input=[x_1,x_2,x_3,\cdots,x_n]$作为输入，同时与它相连的n条线代表了n个输入的权值。特别要注意的是图像隐藏层结点与输出结点还有一个红色的连线，它们代表了**偏置**，即$w_0$

那么结合上篇文章的内容，我们知道图像的i结点，它将输入与对应的权值进行线性加权求和，然后经过$sigmoid$函数计算，把得到的结果作为该个结点的输出。
$$
net_i=w_0+x_1w_1+x_2w_2+x_3w_3+x_nw_n=\sum_{i=0}^{N=n}x_iw_i.(其中x_0=1)\\ o_i=\frac{1}{1+e^{-net_i}}
$$


### 3、反向传播算法（也叫Back Propagation算法或者BP算法）

单个结点用梯度下降的方法，可以去更新权值向量。而对于多层网络结构，我们也可以用类似的方法也推导整个网络的权值更新法则，我们把这种方法叫作反向传播算法，因为它是从输出层开始向前逐层更新权值的。

算法如下：

​	采用迭代的算法来训练整个网络，随机设定初值，计算当前网络的输出，然后根据当前输出和label之间的差去改变前面各层的参数，直到收敛（整体是一个梯度下降法）

### 4、问题

（1）随着隐藏层的增加，参数呈倍数增加，导致计算慢

（2）由梯度扩散（Gradient Diffusion）和局部最小值（Poor Local Minima）问题，导致梯度下降法对包含多个隐藏层的神经网络训练效果不理想。本应用于修正模型参数的误差随着层数的增加指数递减，这导致了模型训练的效率低下

## 三、深度学习

### 1、概览

​	深度学习的基础是机器学习中的分散表示（distributed representation）。分散表示假定观测值是由不同因子相互作用生成。在此基础上，深度学习进一步假定这一相互作用的过程可分为多个层次，代表对观测值的多层抽象。不同的层数和层的规模可用于不同程度的抽象。

​	Deep learning可以理解为neural network的发展。大约二三十年前，neural network曾经是ML领域特别火热的一个方向，但是后来确慢慢淡出了，原因包括以下几个方面：

- 梯度越来越稀疏：从顶层越往下，误差校正信号越来越小；
- 收敛到局部最小值：尤其是从远离最优区域开始的时候（随机值初始化会导致这种情况的发生）；
- 一般，我们只能用有标签的数据来训练：但大部分的数据是没标签的，而大脑可以从没有标签的的数据中学习；

 所以中间有大约20多年的时间，神经网络被关注很少，这段时间基本上是SVM和boosting算法的天下。直到2007年前后，[杰弗里·辛顿](https://zh.wikipedia.org/wiki/%E6%9D%B0%E5%BC%97%E9%87%8C%C2%B7%E8%BE%9B%E9%A1%BF)和鲁斯兰·萨拉赫丁诺夫（Ruslan Salakhutdinov）提出了一种基于无标定数据，通过非监督式学习构建多层神经网络的有效方法，即深度神经网络（Deep Neural Networks, DNN）的训练方法 这一算法将网络中的每一层视为[无监督](https://zh.wikipedia.org/w/index.php?title=%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&action=edit&redlink=1)的[受限玻尔兹曼机](https://zh.wikipedia.org/wiki/%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA)，再使用有监督的反向传播算法进行调优[[19\]](https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0#cite_note-HINTON2007-19)。

区别于传统的浅层学习，深度学习的不同在于：

- 强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；
- 明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。

### 2、学习方法

​	由Geoffrey Hinton等人发明的深度神经网络训练方法包括两步，一是对每一单层中的节点进行逐层预训练（Layer-Wise Pre-Training），二是使用Wake-Sleep算法进行调优（Fine Tuning）[19][27]。

![img](http://mmbiz.qpic.cn/mmbiz_png/3cp2Fa9zElSMQFHzCg3zHaOHxibmyhUjPxaAWgbNbSBIm6Mqupa4EAEfPsfG3h8R0aXU7QgS4R9SGryq1HQn1yg/640?wx_fmt=png&wxfrom=5&wx_lazy=1)



​	最左边的部分描述非监督式的特征学习过程：由多个堆叠的受限玻尔兹曼机逐层重建自己的输入，得到初始人脸图片的另一种表达；中间部分的深度自动编码器得出多次对初始输入的重建与初始人脸图片的误差；最右边的部分描述监督式的调优过程：将对初始人脸图片的重建作为输入，将初始的人脸图片作为已知的期望输出，然后使用反向传播算法不断调整节点收到的输入的权重，直到重建的输入与初始输入的误差低于预定门槛。

​	调优所使用的算法分为醒（Wake）和睡（Sleep）两部分，前者是认知过程，期间反向对生成权重进行调整，后者是生成过程，期间调整前馈的认知权重；深度学习网络中所有的输入和节点的初始权重，并不像传统的机器学习那样是随机分配的，而是通过逐层预训练，即非监督式的特征学习来获得，所以初始权重分配更接近最佳解决方案。

## 四、卷积神经网络

### 1、概览

​	卷积神经网络（Convolutional Neural Network,CNN）是一种目前更容易训练并实现泛化的深度前馈神经网络，应用于图像识别和分类已经被证明非常有效。AlphaGo的策略网络（Policy Networks）和价值网络（Policy Networks）用到的就是深度卷积神经网络。

卷积神经网络还被应用于推荐系统和自然语言处理，其与循环神经网络（Recurrent Neural Network）结合可以做语音处理

### 2、核心点

- **局部感受野**。形象地说，就是模仿你的眼睛，想想看，你在看东西的时候，目光是聚焦在一个相对很小的局部的吧？严格一些说，普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上，而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。举个栗子，依旧是1000×1000的图像，使用10×10的感受野，那么每个神经元只需要100个权值参数；不幸的是，由于需要将输入图像扫描一遍，共需要991×991个神经元！参数数目减少了一个数量级，不过还是太多。

- **权值共享**。形象地说，就如同你的某个神经中枢中的神经细胞，它们的结构、功能是相同的，甚至是可以互相替代的。也就是，在卷积神经网中，同一个卷积核内，所有的神经元的权值是相同的，从而大大减少需要训练的参数。继续上一个栗子，虽然需要991×991个神经元，但是它们的权值是共享的呀，所以还是只需要100个权值参数，以及1个偏置参数。从MLP的 10^9 到这里的100，就是这么狠！作为补充，在CNN中的每个隐藏，一般会有多个卷积核。

- **池化**。形象地说，你先随便看向远方，然后闭上眼睛，你仍然记得看到了些什么，但是你能完全回忆起你刚刚看到的每一个细节吗？同样，在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。

  ​

### 3、结构

### 卷积层

卷积层（Convolutional layer），卷积神经网络中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过[反向传播算法](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95)优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。

### 线性整流层

线性整流层（Rectified Linear Units layer, ReLU layer）使用[线性整流](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0)（Rectified Linear Units, ReLU）![{\displaystyle f(x)=\max(0,x)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5fa5d3598751091eed580bd9dca873f496a2d0ac)作为这一层神经的激活函数（Activation function）。它可以增强判定函数和整个神经网络的非线性特性，而本身并不会改变卷积层。

事实上，其他的一些函数也可以用于增强网络的非线性特性，如[双曲正切函数](https://zh.wikipedia.org/wiki/%E5%8F%8C%E6%9B%B2%E6%AD%A3%E5%88%87%E5%87%BD%E6%95%B0) ![{\displaystyle f(x)=\tanh(x)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1a319ec32dbb0c625fa4802baf9252d1f00854e2),![{\displaystyle f(x)=|\tanh(x)|}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d1eb71e39ce9687851b7ec55bb8f54f42df2a828)，或者[Sigmoid函数](https://zh.wikipedia.org/wiki/S%E5%87%BD%E6%95%B0)![{\displaystyle f(x)=(1+e^{-x})^{-1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6f6e8c1bc5646e39b558bc46f997c5db23471af5)。相比其它函数来说，ReLU函数更受青睐，这是因为它可以将神经网络的训练速度提升数倍[[3\]](https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C#cite_note-3)，而并不会对模型的泛化准确度造成产生显著影响。

### 池化层(Pooling Layer)

[![img](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/314px-Max_pooling.png)](https://zh.wikipedia.org/wiki/File:Max_pooling.png)每隔2个元素进行的2x2最大池化

池化（Pooling）是卷积神经网络中另一个重要的概念，它实际上一种形式的向下采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了[过拟合](https://zh.wikipedia.org/wiki/%E8%BF%87%E6%8B%9F%E5%90%88)。通常来说，CNN的卷积层之间都会周期性地插入池化层。

池化层通常会分别作用于每个输入的特征并减小其大小。目前最常用形式的池化层是每隔2个元素从图像划分出![2\times 2](https://wikimedia.org/api/rest_v1/media/math/render/svg/f8a0e3400ffb97d67c00267ed50cddfe824cbe80)的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。

除了最大池化之外，池化层也可以使用其他池化函数，例如“平均池化”甚至“[L2-范数](https://zh.wikipedia.org/wiki/Lp%E7%A9%BA%E9%97%B4#.E9.95.BF.E5.BA.A6.E3.80.81.E8.B7.9D.E7.A6.BB.E4.B8.8E.E8.8C.83.E6.95.B0)池化”等。过去，平均池化的使用曾经较为广泛，但是最近由于最大池化在实践中的表现更好，平均池化已经不太常用。

由于池化层过快地减少了数据的大小，目前文献中的趋势是使用较小的池化滤镜，[[4\]](https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C#cite_note-4)甚至不再使用池化层。[[5\]](https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C#cite_note-5)

### 损失函数层[[编辑](https://zh.wikipedia.org/w/index.php?title=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&action=edit&section=7)]

损失函数层（loss layer）用于决定训练过程如何来“惩罚”网络的预测结果和真实结果之间的差异，它通常是网络的最后一层。各种不同的损失函数适用于不同类型的任务。例如，[Softmax](https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0)交叉熵损失函数常常被用于在K个类别中选出一个，而[Sigmoid](https://zh.wikipedia.org/wiki/%E9%82%8F%E8%BC%AF%E5%87%BD%E6%95%B8)交叉熵损失函数常常用于多个独立的二分类问题。欧几里得损失函数常常用于结果取值范围为任意实数的问题。

## 五、循环神经网络

### 1、概览

**递归神经网络**（**RNN**）是两种[人工神经网络](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)的总称。一种是**时间递归神经网络**（recurrent neural network），另一种是**结构递归神经网络**（recursive neural network）。时间递归神经网络的神经元间连接构成[有向图](https://zh.wikipedia.org/wiki/%E6%9C%89%E5%90%91%E5%9B%BE)，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。RNN一般指代时间递归神经网络。单纯递归神经网络因为无法处理随着递归，权重指数级爆炸或消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的**LSTM**可以很好解决这个问题。

### 2、结构

不同于传统的FNNs(Feed-forward Neural Networks，前向反馈神经网络)，RNNs引入了定向循环，能够处理那些输入之间前后关联的问题。定向循环结构如下图所示：

![RNN_3](http://img.blog.csdn.net/20150921225622105)



### 3、核心点

- **权值共享**。卷积神经网络是空间结构共享，而递归神经网络则是时间结构共享。参数W,U,V是共享的



# 六、生命游戏

在经过前面一大堆枯燥的数学公式摧残，最后介绍大家一个好玩的游戏：生命游戏。

http://weibo.com/6174606644/F7wvLCqdq?u=6174606644&m=4125372850887510&cu=1702286027&ru=3949671123&rm=4125372162871563&type=comment



**参考文献：**

https://zh.wikipedia.org/wiki/深度学习

https://www.zhihu.com/question/34681168

 https://mp.weixin.qq.com/s?__biz=MjM5NDYxNjc5Ng==&mid=2649704078&idx=1&sn=3309b203f4c9839981265def2cc724cc&scene=1&srcid=0901isLiWiElskKPdubcq7yF#rd

http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi#toc_9

https://zhuanlan.zhihu.com/p/27642620

http://blog.csdn.net/heyongluoyao8/article/details/48636251